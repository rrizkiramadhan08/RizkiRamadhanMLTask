{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate dummy classification data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Step 2: Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)  # Normalize features\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define MLP model\n",
    "def create_mlp(input_size, hidden_layers, hidden_neurons, activation_function):\n",
    "    layers = []\n",
    "    # Input layer to first hidden layer\n",
    "    layers.append(nn.Linear(input_size, hidden_neurons))\n",
    "    \n",
    "    # Add hidden layers\n",
    "    for _ in range(hidden_layers - 1):\n",
    "        layers.append(nn.Linear(hidden_neurons, hidden_neurons))\n",
    "    \n",
    "    # Choose activation function\n",
    "    if activation_function == 'linear':\n",
    "        activation = nn.Identity()\n",
    "    elif activation_function == 'Sigmoid':\n",
    "        activation = nn.Sigmoid()\n",
    "    elif activation_function == 'ReLU':\n",
    "        activation = nn.ReLU()\n",
    "    elif activation_function == 'Softmax':\n",
    "        activation = nn.Softmax(dim=1)\n",
    "    elif activation_function == 'Tanh':\n",
    "        activation = nn.Tanh()\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(nn.Linear(hidden_neurons, 2))  # Output 2 classes (0 or 1)\n",
    "    \n",
    "    # Combine all layers\n",
    "    model = nn.Sequential(*layers)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Train and evaluate the model\n",
    "def train_and_evaluate(model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size, epochs, learning_rate):\n",
    "    # DataLoader for batch training\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Optimizer (Adam) and loss function (CrossEntropyLoss for classification)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset gradients\n",
    "            output = model(X_batch)  # Forward pass\n",
    "            loss = criterion(output, y_batch)  # Compute loss\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Evaluation after training\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        # Training accuracy\n",
    "        train_output = model(X_train_tensor)\n",
    "        train_pred = torch.argmax(train_output, dim=1)  # Get predicted class\n",
    "        train_accuracy = accuracy_score(y_train_tensor.numpy(), train_pred.numpy())  # Compute train accuracy\n",
    "        train_loss = criterion(train_output, y_train_tensor).item()  # Compute train loss\n",
    "\n",
    "        # Test accuracy\n",
    "        test_output = model(X_test_tensor)\n",
    "        test_pred = torch.argmax(test_output, dim=1)  # Get predicted class\n",
    "        test_accuracy = accuracy_score(y_test_tensor.numpy(), test_pred.numpy())  # Compute test accuracy\n",
    "        test_loss = criterion(test_output, y_test_tensor).item()  # Compute test loss\n",
    "\n",
    "    return train_accuracy, train_loss, test_accuracy, test_loss  # Return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.1 learning rate, 64 batch size\n",
      "Train Accuracy: 88.43%\n",
      "Train Loss: 0.3108\n",
      "Test Accuracy: 84.67%\n",
      "Test Loss: 0.3900\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.1 learning rate, 128 batch size\n",
      "Train Accuracy: 88.71%\n",
      "Train Loss: 0.3085\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3832\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.1 learning rate, 256 batch size\n",
      "Train Accuracy: 88.43%\n",
      "Train Loss: 0.3080\n",
      "Test Accuracy: 84.33%\n",
      "Test Loss: 0.3880\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.1 learning rate, 512 batch size\n",
      "Train Accuracy: 89.00%\n",
      "Train Loss: 0.3107\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3947\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.01 learning rate, 64 batch size\n",
      "Train Accuracy: 89.14%\n",
      "Train Loss: 0.3077\n",
      "Test Accuracy: 84.67%\n",
      "Test Loss: 0.3840\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.01 learning rate, 128 batch size\n",
      "Train Accuracy: 88.14%\n",
      "Train Loss: 0.3077\n",
      "Test Accuracy: 83.67%\n",
      "Test Loss: 0.3779\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.01 learning rate, 256 batch size\n",
      "Train Accuracy: 88.43%\n",
      "Train Loss: 0.3072\n",
      "Test Accuracy: 84.67%\n",
      "Test Loss: 0.3840\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.01 learning rate, 512 batch size\n",
      "Train Accuracy: 88.86%\n",
      "Train Loss: 0.3082\n",
      "Test Accuracy: 85.67%\n",
      "Test Loss: 0.3860\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.001 learning rate, 64 batch size\n",
      "Train Accuracy: 88.57%\n",
      "Train Loss: 0.3551\n",
      "Test Accuracy: 84.67%\n",
      "Test Loss: 0.4023\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.001 learning rate, 128 batch size\n",
      "Train Accuracy: 88.00%\n",
      "Train Loss: 0.4241\n",
      "Test Accuracy: 84.67%\n",
      "Test Loss: 0.4525\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.001 learning rate, 256 batch size\n",
      "Train Accuracy: 85.14%\n",
      "Train Loss: 0.5209\n",
      "Test Accuracy: 81.33%\n",
      "Test Loss: 0.5308\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.001 learning rate, 512 batch size\n",
      "Train Accuracy: 51.29%\n",
      "Train Loss: 0.7328\n",
      "Test Accuracy: 50.67%\n",
      "Test Loss: 0.7012\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.0001 learning rate, 64 batch size\n",
      "Train Accuracy: 43.86%\n",
      "Train Loss: 0.7354\n",
      "Test Accuracy: 43.00%\n",
      "Test Loss: 0.7310\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.0001 learning rate, 128 batch size\n",
      "Train Accuracy: 50.71%\n",
      "Train Loss: 0.7494\n",
      "Test Accuracy: 48.33%\n",
      "Test Loss: 0.7747\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.0001 learning rate, 256 batch size\n",
      "Train Accuracy: 58.43%\n",
      "Train Loss: 0.6670\n",
      "Test Accuracy: 58.00%\n",
      "Test Loss: 0.6761\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 25 epochs, 0.0001 learning rate, 512 batch size\n",
      "Train Accuracy: 52.00%\n",
      "Train Loss: 0.7080\n",
      "Test Accuracy: 54.67%\n",
      "Test Loss: 0.7032\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.1 learning rate, 64 batch size\n",
      "Train Accuracy: 88.00%\n",
      "Train Loss: 0.3243\n",
      "Test Accuracy: 83.67%\n",
      "Test Loss: 0.4187\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.1 learning rate, 128 batch size\n",
      "Train Accuracy: 88.57%\n",
      "Train Loss: 0.3090\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3844\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.1 learning rate, 256 batch size\n",
      "Train Accuracy: 88.14%\n",
      "Train Loss: 0.3086\n",
      "Test Accuracy: 86.67%\n",
      "Test Loss: 0.3796\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.1 learning rate, 512 batch size\n",
      "Train Accuracy: 88.86%\n",
      "Train Loss: 0.3101\n",
      "Test Accuracy: 87.00%\n",
      "Test Loss: 0.3830\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.01 learning rate, 64 batch size\n",
      "Train Accuracy: 88.29%\n",
      "Train Loss: 0.3074\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3811\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.01 learning rate, 128 batch size\n",
      "Train Accuracy: 89.00%\n",
      "Train Loss: 0.3073\n",
      "Test Accuracy: 85.33%\n",
      "Test Loss: 0.3802\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.01 learning rate, 256 batch size\n",
      "Train Accuracy: 88.00%\n",
      "Train Loss: 0.3071\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3810\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.01 learning rate, 512 batch size\n",
      "Train Accuracy: 88.43%\n",
      "Train Loss: 0.3076\n",
      "Test Accuracy: 84.33%\n",
      "Test Loss: 0.3821\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.001 learning rate, 64 batch size\n",
      "Train Accuracy: 88.57%\n",
      "Train Loss: 0.3109\n",
      "Test Accuracy: 85.67%\n",
      "Test Loss: 0.3769\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.001 learning rate, 128 batch size\n",
      "Train Accuracy: 88.57%\n",
      "Train Loss: 0.3569\n",
      "Test Accuracy: 85.00%\n",
      "Test Loss: 0.3986\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.001 learning rate, 256 batch size\n",
      "Train Accuracy: 86.57%\n",
      "Train Loss: 0.4378\n",
      "Test Accuracy: 86.00%\n",
      "Test Loss: 0.4670\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.001 learning rate, 512 batch size\n",
      "Train Accuracy: 85.86%\n",
      "Train Loss: 0.4602\n",
      "Test Accuracy: 83.67%\n",
      "Test Loss: 0.4769\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.0001 learning rate, 64 batch size\n",
      "Train Accuracy: 72.43%\n",
      "Train Loss: 0.5734\n",
      "Test Accuracy: 67.33%\n",
      "Test Loss: 0.5926\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.0001 learning rate, 128 batch size\n",
      "Train Accuracy: 54.00%\n",
      "Train Loss: 0.6942\n",
      "Test Accuracy: 50.00%\n",
      "Test Loss: 0.7116\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.0001 learning rate, 256 batch size\n",
      "Train Accuracy: 46.71%\n",
      "Train Loss: 0.7333\n",
      "Test Accuracy: 45.33%\n",
      "Test Loss: 0.7422\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 50 epochs, 0.0001 learning rate, 512 batch size\n",
      "Train Accuracy: 48.71%\n",
      "Train Loss: 0.7575\n",
      "Test Accuracy: 47.67%\n",
      "Test Loss: 0.7579\n",
      "\n",
      "Training with 1 hidden layers, 4 neurons, linear activation, 100 epochs, 0.1 learning rate, 64 batch size\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Experiment with hyperparameters\n",
    "hidden_layers_options = [1, 2, 3]  # Number of hidden layers\n",
    "hidden_neurons_options = [4, 8, 16, 32, 64]  # Number of neurons in each hidden layer\n",
    "activation_functions = ['linear', 'Sigmoid', 'ReLU', 'Softmax', 'Tanh']  # Activation functions\n",
    "epochs_options = [25, 50, 100, 250]  # Number of epochs\n",
    "learning_rates = [0.1, 0.01, 0.001, 0.0001]  # Learning rates\n",
    "batch_sizes = [ 64, 128, 256, 512]  # Batch sizes\n",
    "\n",
    "# Tracking results\n",
    "results = []  # To store the results of each experiment\n",
    "best_result = None\n",
    "worst_result = None\n",
    "best_per_activation = {activation: None for activation in activation_functions}  # Track best result per activation function\n",
    "\n",
    "# Loop through all combinations of hyperparameters\n",
    "for hidden_layers in hidden_layers_options:\n",
    "    for hidden_neurons in hidden_neurons_options:\n",
    "        for activation_function in activation_functions:\n",
    "            for epochs in epochs_options:\n",
    "                for learning_rate in learning_rates:\n",
    "                    for batch_size in batch_sizes:\n",
    "                        # Print the current experiment configuration\n",
    "                        print(f\"Training with {hidden_layers} hidden layers, {hidden_neurons} neurons, {activation_function} activation, \"\n",
    "                              f\"{epochs} epochs, {learning_rate} learning rate, {batch_size} batch size\")\n",
    "\n",
    "                        # Create the model for each combination of hyperparameters\n",
    "                        model = create_mlp(X_train_tensor.shape[1], hidden_layers, hidden_neurons, activation_function)\n",
    "                        \n",
    "                        # Train and evaluate the model\n",
    "                        train_acc, train_loss, test_acc, test_loss = train_and_evaluate(\n",
    "                            model, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, batch_size, epochs, learning_rate\n",
    "                        )\n",
    "\n",
    "                        # Store the results\n",
    "                        results.append({\n",
    "                            'Hidden Layers': hidden_layers,\n",
    "                            'Hidden Neurons': hidden_neurons,\n",
    "                            'Activation Function': activation_function,\n",
    "                            'Epochs': epochs,\n",
    "                            'Learning Rate': learning_rate,\n",
    "                            'Batch Size': batch_size,\n",
    "                            'Train Accuracy': train_acc,\n",
    "                            'Train Loss': train_loss,\n",
    "                            'Test Accuracy': test_acc,\n",
    "                            'Test Loss': test_loss\n",
    "                        })\n",
    "\n",
    "                        # Print final results for this experiment\n",
    "                        print(f\"Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "                        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "                        print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "                        print(f\"Test Loss: {test_loss:.4f}\\n\")\n",
    "\n",
    "                        # Update best and worst results\n",
    "                        if best_result is None or test_acc > best_result['Test Accuracy']:\n",
    "                            best_result = {\n",
    "                                'Hyperparameters': {\n",
    "                                    'Hidden Layers': hidden_layers,\n",
    "                                    'Hidden Neurons': hidden_neurons,\n",
    "                                    'Activation Function': activation_function,\n",
    "                                    'Epochs': epochs,\n",
    "                                    'Learning Rate': learning_rate,\n",
    "                                    'Batch Size': batch_size\n",
    "                                },\n",
    "                                'Test Accuracy': test_acc\n",
    "                            }\n",
    "                        if worst_result is None or test_acc < worst_result['Test Accuracy']:\n",
    "                            worst_result = {\n",
    "                                'Hyperparameters': {\n",
    "                                    'Hidden Layers': hidden_layers,\n",
    "                                    'Hidden Neurons': hidden_neurons,\n",
    "                                    'Activation Function': activation_function,\n",
    "                                    'Epochs': epochs,\n",
    "                                    'Learning Rate': learning_rate,\n",
    "                                    'Batch Size': batch_size\n",
    "                                },\n",
    "                                'Test Accuracy': test_acc\n",
    "                            }\n",
    "\n",
    "                        # Track the best configuration for each activation function\n",
    "                        if best_per_activation[activation_function] is None or test_acc > best_per_activation[activation_function]['Test Accuracy']:\n",
    "                            best_per_activation[activation_function] = {\n",
    "                                'Hyperparameters': {\n",
    "                                    'Hidden Layers': hidden_layers,\n",
    "                                    'Hidden Neurons': hidden_neurons,\n",
    "                                    'Activation Function': activation_function,\n",
    "                                    'Epochs': epochs,\n",
    "                                    'Learning Rate': learning_rate,\n",
    "                                    'Batch Size': batch_size\n",
    "                                },\n",
    "                                'Test Accuracy': test_acc\n",
    "                            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Hyperparameter Configuration (Overall):\n",
      "Hidden Layers: 1\n",
      "Hidden Neurons: 4\n",
      "Activation Function: linear\n",
      "Epochs: 25\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 32\n",
      "Test Accuracy: 86.00%\n",
      "\n",
      "Worst Hyperparameter Configuration:\n",
      "Hidden Layers: 1\n",
      "Hidden Neurons: 4\n",
      "Activation Function: linear\n",
      "Epochs: 1\n",
      "Learning Rate: 10\n",
      "Batch Size: 64\n",
      "Test Accuracy: 30.33%\n",
      "\n",
      "Best Configuration for Activation Function linear:\n",
      "Hidden Layers: 1\n",
      "Hidden Neurons: 4\n",
      "Activation Function: linear\n",
      "Epochs: 25\n",
      "Learning Rate: 0.001\n",
      "Batch Size: 32\n",
      "Test Accuracy: 86.00%\n",
      "\n",
      "Best Configuration for Activation Function Sigmoid:\n",
      "No result for this activation function.\n",
      "\n",
      "Best Configuration for Activation Function ReLU:\n",
      "No result for this activation function.\n",
      "\n",
      "Best Configuration for Activation Function Softmax:\n",
      "No result for this activation function.\n",
      "\n",
      "Best Configuration for Activation Function Tanh:\n",
      "No result for this activation function.\n"
     ]
    }
   ],
   "source": [
    "# Display the best and worst configurations\n",
    "print(\"\\nBest Hyperparameter Configuration (Overall):\")\n",
    "print(f\"Hidden Layers: {best_result['Hyperparameters']['Hidden Layers']}\")\n",
    "print(f\"Hidden Neurons: {best_result['Hyperparameters']['Hidden Neurons']}\")\n",
    "print(f\"Activation Function: {best_result['Hyperparameters']['Activation Function']}\")\n",
    "print(f\"Epochs: {best_result['Hyperparameters']['Epochs']}\")\n",
    "print(f\"Learning Rate: {best_result['Hyperparameters']['Learning Rate']}\")\n",
    "print(f\"Batch Size: {best_result['Hyperparameters']['Batch Size']}\")\n",
    "print(f\"Test Accuracy: {best_result['Test Accuracy'] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nWorst Hyperparameter Configuration:\")\n",
    "print(f\"Hidden Layers: {worst_result['Hyperparameters']['Hidden Layers']}\")\n",
    "print(f\"Hidden Neurons: {worst_result['Hyperparameters']['Hidden Neurons']}\")\n",
    "print(f\"Activation Function: {worst_result['Hyperparameters']['Activation Function']}\")\n",
    "print(f\"Epochs: {worst_result['Hyperparameters']['Epochs']}\")\n",
    "print(f\"Learning Rate: {worst_result['Hyperparameters']['Learning Rate']}\")\n",
    "print(f\"Batch Size: {worst_result['Hyperparameters']['Batch Size']}\")\n",
    "print(f\"Test Accuracy: {worst_result['Test Accuracy'] * 100:.2f}%\")\n",
    "\n",
    "# Display best configuration for each activation function\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nBest Configuration for Activation Function {activation}:\")\n",
    "    if best_per_activation[activation]:\n",
    "        print(f\"Hidden Layers: {best_per_activation[activation]['Hyperparameters']['Hidden Layers']}\")\n",
    "        print(f\"Hidden Neurons: {best_per_activation[activation]['Hyperparameters']['Hidden Neurons']}\")\n",
    "        print(f\"Activation Function: {activation}\")\n",
    "        print(f\"Epochs: {best_per_activation[activation]['Hyperparameters']['Epochs']}\")\n",
    "        print(f\"Learning Rate: {best_per_activation[activation]['Hyperparameters']['Learning Rate']}\")\n",
    "        print(f\"Batch Size: {best_per_activation[activation]['Hyperparameters']['Batch Size']}\")\n",
    "        print(f\"Test Accuracy: {best_per_activation[activation]['Test Accuracy'] * 100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No result for this activation function.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
