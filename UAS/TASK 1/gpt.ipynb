{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nXaUNIAKcxHH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## GPT menggunakan scretch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "d267e15c-526d-463b-f572-6e35bf2ae87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-04 13:25:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-01-04 13:25:36 (12.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# Membuka file 'input.txt' dalam mode baca ('r') dan dengan encoding 'utf-8' untuk memastikan karakter non-ASCII dapat dibaca dengan benar\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    # Membaca seluruh isi file dan menyimpannya dalam variabel 'text'\n",
        "    text = f.read()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "5c14c0c6-aad4-4cd3-a52d-486224b13f59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan panjang dataset dalam jumlah karakter\n",
        "print(\"length of dataset in characters: \", len(text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "3e0f2291-9ad3-4252-8bd1-511c388a2676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Menampilkan 1000 karakter pertama dari isi dataset\n",
        "print(text[:1000])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "dbabf3f4-5285-41e9-9c2d-cb027e1e99bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# Mengambil semua karakter unik yang terdapat dalam teks\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# Menentukan jumlah karakter unik (ukuran kosakata)\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mencetak semua karakter unik yang ada dalam teks\n",
        "print(''.join(chars))\n",
        "\n",
        "# Mencetak ukuran kosakata (jumlah karakter unik)\n",
        "print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "6ddb02b0-c60f-4674-afde-b1e6f14abe82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[49, 39, 51, 59, 1, 57, 43, 42, 39, 52, 45, 1, 51, 43, 52, 45, 45, 59, 52, 39, 49, 39, 52, 1, 28, 63, 32, 53, 56, 41, 46]\n",
            "kamu sedang menggunakan PyTorch\n"
          ]
        }
      ],
      "source": [
        "# Membuat pemetaan dari karakter ke integer\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }  # Pemetaan dari karakter (ch) ke indeks (i)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }  # Pemetaan dari indeks (i) ke karakter (ch)\n",
        "\n",
        "# Membuat fungsi encoder: mengubah string menjadi daftar integer berdasarkan pemetaan\n",
        "encode = lambda s: [stoi[c] for c in s]  # Menggunakan pemetaan 'stoi' untuk mengonversi setiap karakter menjadi integer\n",
        "\n",
        "# Membuat fungsi decoder: mengubah daftar integer kembali menjadi string\n",
        "decode = lambda l: ''.join([itos[i] for i in l])  # Menggunakan pemetaan 'itos' untuk mengonversi setiap integer kembali ke karakter\n",
        "\n",
        "# Menampilkan hasil konversi string \"hii there\" menjadi daftar integer\n",
        "print(encode(\"kamu sedang menggunakan PyTorch\"))\n",
        "\n",
        "# Menampilkan hasil dekripsi kembali dari daftar integer ke string yang semula\n",
        "print(decode(encode(\"kamu sedang menggunakan PyTorch\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "973c9370-ed17-4ed2-8db2-fef7074a0e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# Mengimpor pustaka PyTorch untuk manipulasi tensor\n",
        "import torch  # PyTorch digunakan untuk operasi tensor dan komputasi yang efisien\n",
        "\n",
        "# Mengencode seluruh dataset teks dan mengubahnya menjadi tensor PyTorch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)  # Mengonversi hasil encode ke tensor PyTorch dengan tipe data 'long'\n",
        "\n",
        "# Menampilkan dimensi (shape) dan tipe data dari tensor\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# Menampilkan 1000 elemen pertama dari tensor untuk inspeksi\n",
        "print(data[:1000])  # Menampilkan 1000 karakter pertama yang telah diencode menjadi bentuk tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Membagi data menjadi set pelatihan dan validasi\n",
        "n = int(0.9*len(data))  # Menentukan indeks pemisahan: 90% data digunakan untuk pelatihan, sisanya untuk validasi\n",
        "\n",
        "# Menyusun data pelatihan yang mencakup 90% pertama dari data\n",
        "train_data = data[:n]\n",
        "\n",
        "# Menyusun data validasi yang mencakup sisanya (10% terakhir) dari data\n",
        "val_data = data[n:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menentukan ukuran blok (jumlah karakter dalam satu blok)\n",
        "block_size = 8\n",
        "\n",
        "# Menampilkan blok pertama yang berisi 'block_size + 1' karakter dari data pelatihan\n",
        "train_data[:block_size+1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "# Membagi data pelatihan menjadi input (x) dan target (y) untuk model\n",
        "x = train_data[:block_size]  # Mengambil blok pertama data sebagai input (x), berisi 8 karakter pertama\n",
        "y = train_data[1:block_size+1]  # Mengambil target yang merupakan karakter yang diikuti oleh input, berisi 8 karakter setelah x\n",
        "\n",
        "# Iterasi melalui setiap posisi dalam blok untuk menampilkan konteks dan target\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]  # Mengambil bagian awal dari input yang berisi konteks hingga posisi t\n",
        "    target = y[t]  # Target untuk posisi t adalah karakter yang sesuai pada y\n",
        "    print(f\"when input is {context} the target: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "# Menetapkan seed untuk kontrol acak agar hasil eksperimen dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Menentukan ukuran batch (berapa banyak urutan independen yang akan diproses secara paralel)\n",
        "batch_size = 4\n",
        "\n",
        "# Menentukan ukuran blok (panjang konteks maksimum untuk prediksi)\n",
        "block_size = 8\n",
        "\n",
        "# Fungsi untuk mendapatkan batch data\n",
        "def get_batch(split):\n",
        "    # Menghasilkan batch kecil data input x dan target y\n",
        "    # Memilih data pelatihan atau validasi berdasarkan argumen split\n",
        "    data = train_data if split == 'train' else val_data\n",
        "\n",
        "    # Menghasilkan indeks acak untuk memulai setiap urutan input dalam batch\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))  # Pastikan urutan tidak melampaui panjang data\n",
        "\n",
        "    # Membuat tensor input x dari setiap urutan data\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])  # Setiap urutan memiliki panjang block_size\n",
        "\n",
        "    # Membuat tensor target y yang merupakan urutan data yang bergeser satu langkah\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])  # Target y adalah urutan input yang bergeser satu karakter\n",
        "    return x, y\n",
        "\n",
        "# Mendapatkan batch pelatihan\n",
        "xb, yb = get_batch('train')\n",
        "\n",
        "# Menampilkan bentuk dan isi dari input dan target batch\n",
        "print('inputs:')\n",
        "print(xb.shape)  # Menampilkan bentuk (dimensi) dari tensor input\n",
        "print(xb)  # Menampilkan nilai dari tensor input\n",
        "print('targets:')\n",
        "print(yb.shape)  # Menampilkan bentuk (dimensi) dari tensor target\n",
        "print(yb)  # Menampilkan nilai dari tensor target\n",
        "\n",
        "print('----')\n",
        "\n",
        "# Menampilkan input dan target untuk setiap urutan dalam batch\n",
        "for b in range(batch_size):  # Iterasi untuk setiap batch (urutan input)\n",
        "    for t in range(block_size):  # Iterasi untuk setiap posisi dalam urutan\n",
        "        context = xb[b, :t+1]  # Mengambil konteks yang semakin panjang dari input untuk setiap langkah waktu\n",
        "        target = yb[b,t]  # Menentukan target yang sesuai untuk posisi t dalam urutan\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "# Mencetak tensor input (xb) yang akan digunakan sebagai input untuk transformer\n",
        "print(xb)  # Menampilkan seluruh tensor input dari batch yang dihasilkan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ],
      "source": [
        "# Mengimpor pustaka PyTorch dan fungsionalitas yang dibutuhkan\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Menetapkan seed untuk kontrol acak agar hasil eksperimen dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Definisi kelas model bahasa bigram menggunakan PyTorch\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Membuat tabel embedding untuk token. Tabel ini mengasosiasikan setiap token dengan logit untuk token berikutnya.\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)  # Ukuran embedding adalah vocab_size, karena kita memprediksi token berikutnya\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx dan targets adalah tensor (B, T) yang berisi indeks token dalam urutan (dimensi B= batch, T= waktu/urutan)\n",
        "        logits = self.token_embedding_table(idx)  # (B, T, C), logit untuk setiap token pada setiap posisi waktu\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None  # Jika tidak ada target, tidak menghitung loss\n",
        "        else:\n",
        "            B, T, C = logits.shape  # Mengambil bentuk dari logits (batch_size, seq_length, vocab_size)\n",
        "            logits = logits.view(B*T, C)  # Merubah bentuk logits menjadi dua dimensi: (B*T, C)\n",
        "            targets = targets.view(B*T)  # Merubah target menjadi bentuk (B*T,)\n",
        "            loss = F.cross_entropy(logits, targets)  # Menghitung loss menggunakan cross entropy\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx adalah tensor (B, T) yang berisi urutan indeks dalam konteks saat ini\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Mendapatkan prediksi logit\n",
        "            logits, loss = self(idx)\n",
        "            # Fokus pada langkah waktu terakhir\n",
        "            logits = logits[:, -1, :]  # Hanya ambil logit untuk token terakhir di setiap urutan\n",
        "            # Terapkan softmax untuk mendapatkan probabilitas\n",
        "            probs = F.softmax(logits, dim=-1)  # Menghitung distribusi probabilitas untuk token berikutnya\n",
        "            # Sampling dari distribusi probabilitas\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sampling satu token dari distribusi probabilitas\n",
        "            # Menambahkan token yang disampling ke urutan yang sedang berlangsung\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Menambahkan token baru ke urutan input untuk iterasi berikutnya\n",
        "        return idx\n",
        "\n",
        "# Membuat model dengan ukuran kosakata tertentu\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "\n",
        "# Menghitung logits dan loss untuk input batch yang diberikan\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)  # Menampilkan bentuk dari tensor logits (B, T, C)\n",
        "print(loss)  # Menampilkan loss yang dihitung untuk batch\n",
        "\n",
        "# Men-generate 100 token baru berdasarkan urutan input awal\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# Membuat optimizer PyTorch menggunakan AdamW\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.65630578994751\n"
          ]
        }
      ],
      "source": [
        "# Menetapkan ukuran batch\n",
        "batch_size = 32\n",
        "\n",
        "# Melakukan pelatihan selama 100 langkah\n",
        "for steps in range(100):  # Menambah jumlah langkah untuk hasil yang lebih baik...\n",
        "\n",
        "    # Mengambil batch data pelatihan\n",
        "    xb, yb = get_batch('train')  # Mendapatkan batch input dan target dari data pelatihan\n",
        "\n",
        "    # Mengevaluasi loss (kerugian)\n",
        "    logits, loss = m(xb, yb)  # Menghitung logits dan loss menggunakan model\n",
        "\n",
        "    # Mengatur gradien ke nol sebelum backpropagation\n",
        "    optimizer.zero_grad(set_to_none=True)  # Menghapus gradien sebelumnya agar tidak menumpuk\n",
        "\n",
        "    # Melakukan backpropagation untuk menghitung gradien\n",
        "    loss.backward()  # Menghitung gradien berdasarkan loss\n",
        "\n",
        "    # Memperbarui parameter model berdasarkan gradien\n",
        "    optimizer.step()  # Melakukan langkah optimasi untuk memperbarui bobot model\n",
        "\n",
        "# Mencetak nilai loss setelah selesai pelatihan\n",
        "print(loss.item())  # Menampilkan nilai loss terakhir setelah 100 langkah pelatihan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "oTo.JUZ!!zqe!\n",
            "xBP qbs$Gy'AcOmrLwwt\n",
            "p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\n",
            "rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\n",
            "ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\n",
            "SV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\n",
            "tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\n",
            "pSPYgCuCJrIFtb\n",
            "jQXg\n",
            "pA.P LP,SPJi\n",
            "DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\n",
            "D.?\n"
          ]
        }
      ],
      "source": [
        "# Menghasilkan 500 token baru berdasarkan urutan input yang dimulai dari tensor berisi nilai nol\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# Menetapkan seed untuk kontrol acak agar hasil eksperimen dapat direproduksi\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Membuat matriks bawah segitiga (lower triangular matrix) berukuran 3x3 yang diisi dengan angka 1\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "\n",
        "# Menormalkan setiap baris dari matriks 'a' sehingga jumlah elemen dalam setiap baris menjadi 1\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "\n",
        "# Membuat matriks acak 'b' berukuran 3x2 dengan nilai antara 0 dan 10\n",
        "b = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "# Melakukan perkalian matriks antara 'a' dan 'b'\n",
        "c = a @ b\n",
        "\n",
        "# Menampilkan matriks 'a'\n",
        "print('a=')\n",
        "print(a)\n",
        "\n",
        "# Menampilkan matriks pemisah\n",
        "print('--')\n",
        "\n",
        "# Menampilkan matriks 'b'\n",
        "print('b=')\n",
        "print(b)\n",
        "\n",
        "# Menampilkan hasil perkalian matriks 'a' dan 'b' dalam matriks 'c'\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menetapkan seed untuk kontrol acak agar hasil eksperimen dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Menentukan dimensi tensor untuk batch, waktu (time steps), dan saluran (channels)\n",
        "B, T, C = 4, 8, 2  # B = batch size, T = time steps, C = number of channels\n",
        "\n",
        "# Membuat tensor acak berukuran (B, T, C) yang berisi data dengan distribusi normal\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Menampilkan bentuk tensor x\n",
        "x.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# Membuat tensor kosong 'xbow' yang berukuran (B, T, C) untuk menampung hasil perhitungan rata-rata\n",
        "xbow = torch.zeros((B, T, C))\n",
        "\n",
        "# Iterasi untuk setiap batch (b) dan setiap langkah waktu (t)\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        # Mengambil elemen dari tensor x untuk batch b dan langkah waktu hingga t\n",
        "        xprev = x[b, :t+1]  # (t+1, C), memilih semua langkah waktu hingga t untuk batch b\n",
        "\n",
        "        # Menghitung rata-rata sepanjang dimensi waktu (t) untuk batch b dan langkah waktu t\n",
        "        xbow[b, t] = torch.mean(xprev, 0)  # Mengambil rata-rata sepanjang dimensi waktu (t) untuk setiap saluran (C)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Membuat matriks bobot 'wei' berbentuk segitiga bawah (lower triangular matrix) dengan dimensi T x T\n",
        "wei = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah yang diisi dengan angka 1\n",
        "\n",
        "# Menormalkan setiap baris matriks 'wei' agar jumlah elemen dalam setiap baris menjadi 1\n",
        "wei = wei / wei.sum(1, keepdim=True)  # Normalisasi sehingga jumlah setiap baris adalah 1\n",
        "\n",
        "# Melakukan perkalian matriks antara 'wei' (T, T) dan 'x' (B, T, C)\n",
        "xbow2 = wei @ x  # Perkalian matriks: (T, T) @ (B, T, C) ---> (B, T, C)\n",
        "\n",
        "# Memeriksa apakah hasil dari perkalian matriks sama dengan hasil sebelumnya 'xbow'\n",
        "torch.allclose(xbow, xbow2)  # Memeriksa apakah 'xbow' dan 'xbow2' hampir identik (mendekati sama dalam nilai)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Membuat matriks segitiga bawah (lower triangular matrix) dengan ukuran T x T\n",
        "tril = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah dengan elemen 1 di bawah diagonal dan 0 di atas diagonal\n",
        "\n",
        "# Membuat matriks bobot 'wei' yang berukuran T x T dan menginisialisasinya dengan nol\n",
        "wei = torch.zeros((T, T))\n",
        "\n",
        "# Mengisi elemen di atas diagonal dengan nilai negatif tak hingga (-inf) untuk mask\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # Menandai elemen yang tidak ada di segitiga bawah dengan -inf\n",
        "\n",
        "# Menghitung Softmax dari matriks bobot 'wei' sepanjang dimensi waktu (dim=-1) untuk normalisasi\n",
        "wei = F.softmax(wei, dim=-1)  # Softmax digunakan untuk mengonversi nilai ke probabilitas dengan memastikan jumlah barisnya adalah 1\n",
        "\n",
        "# Melakukan perkalian matriks antara 'wei' dan 'x' untuk mendapatkan hasil agregasi berbobot\n",
        "xbow3 = wei @ x  # Perkalian matriks: (T, T) @ (B, T, C) ---> (B, T, C)\n",
        "\n",
        "# Memeriksa apakah hasil perhitungan menggunakan Softmax hampir sama dengan hasil perhitungan sebelumnya 'xbow'\n",
        "torch.allclose(xbow, xbow3)  # Memeriksa apakah 'xbow' dan 'xbow3' hampir identik (mendekati sama dalam nilai)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Menetapkan seed untuk kontrol acak agar hasil eksperimen dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Menentukan dimensi tensor untuk batch, waktu (time steps), dan saluran (channels)\n",
        "B, T, C = 4, 8, 32  # B = batch size, T = time steps, C = number of channels\n",
        "\n",
        "# Membuat tensor acak 'x' dengan ukuran (B, T, C) yang berisi data dengan distribusi normal\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Menentukan ukuran untuk setiap kepala dalam mekanisme self-attention\n",
        "head_size = 16\n",
        "\n",
        "# Membuat lapisan Linear untuk key, query, dan value\n",
        "key = nn.Linear(C, head_size, bias=False)  # Mengubah tensor input menjadi key dengan dimensi 'head_size'\n",
        "query = nn.Linear(C, head_size, bias=False)  # Mengubah tensor input menjadi query dengan dimensi 'head_size'\n",
        "value = nn.Linear(C, head_size, bias=False)  # Mengubah tensor input menjadi value dengan dimensi 'head_size'\n",
        "\n",
        "# Menghitung key, query, dan value untuk input x\n",
        "k = key(x)   # (B, T, head_size) ---> (B, T, 16)\n",
        "q = query(x) # (B, T, head_size) ---> (B, T, 16)\n",
        "\n",
        "# Menghitung matriks bobot perhatian (attention weights) menggunakan produk titik antara query dan key\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\n",
        "\n",
        "# Membuat matriks segitiga bawah (lower triangular matrix) untuk mask\n",
        "tril = torch.tril(torch.ones(T, T))  # Membuat matriks segitiga bawah berukuran T x T yang diisi dengan angka 1\n",
        "\n",
        "# Masking nilai di atas diagonal agar tidak dipertimbangkan dalam perhatian\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))  # Menandai elemen di atas diagonal dengan -inf\n",
        "\n",
        "# Menghitung softmax dari bobot perhatian untuk mendapatkan distribusi probabilitas\n",
        "wei = F.softmax(wei, dim=-1)  # Softmax memastikan jumlah setiap baris adalah 1\n",
        "\n",
        "# Menghitung output dengan menggunakan bobot perhatian untuk menggabungkan value\n",
        "v = value(x)  # (B, T, head_size)\n",
        "out = wei @ v  # Perkalian matriks antara bobot perhatian 'wei' dan value 'v'\n",
        "\n",
        "# Menampilkan bentuk tensor output dari self-attention\n",
        "out.shape  # Memeriksa bentuk tensor output yang dihasilkan oleh self-attention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "# Membuat tensor acak untuk key dan query dengan ukuran (B, T, head_size)\n",
        "k = torch.randn(B, T, head_size)  # Key tensor dengan dimensi (B, T, head_size)\n",
        "q = torch.randn(B, T, head_size)  # Query tensor dengan dimensi (B, T, head_size)\n",
        "\n",
        "# Menghitung matriks bobot perhatian (attention weights) antara query dan key\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # menyesuaikan tingkat ketajaman distribusi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d:  # Class ini adalah implementasi dari Layer Normalization 1D (sebelumnya BatchNorm1d)\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        self.eps = eps  # epsilon untuk stabilitas numerik, mencegah pembagian dengan nol\n",
        "        self.gamma = torch.ones(dim)  # Gamma adalah parameter skala (scale), diinisialisasi dengan 1\n",
        "        self.beta = torch.zeros(dim)  # Beta adalah parameter pergeseran (shift), diinisialisasi dengan 0\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Melakukan perhitungan untuk forward pass\n",
        "        xmean = x.mean(1, keepdim=True)  # Menghitung rata-rata batch (mean) sepanjang dimensi 1 (dimensi fitur)\n",
        "        xvar = x.var(1, keepdim=True)    # Menghitung varians batch (variance) sepanjang dimensi 1\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalisasi, mengurangi mean dan membagi dengan deviasi standar\n",
        "        self.out = self.gamma * xhat + self.beta  # Menambahkan skala (gamma) dan pergeseran (beta) ke hasil normalisasi\n",
        "        return self.out  # Mengembalikan output yang sudah dinormalisasi dan ditransformasi\n",
        "\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]  # Mengembalikan parameter model (gamma dan beta)\n",
        "\n",
        "# Menetapkan seed untuk kontrol acak agar eksperimen dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Membuat instance dari LayerNorm1d dengan dimensi fitur 100\n",
        "module = LayerNorm1d(100)\n",
        "\n",
        "# Membuat tensor acak dengan ukuran batch 32 dan 100 dimensi untuk setiap vektor\n",
        "x = torch.randn(32, 100)\n",
        "\n",
        "# Mengaplikasikan LayerNorm1d pada tensor x\n",
        "x = module(x)\n",
        "\n",
        "# Menampilkan bentuk tensor output setelah normalisasi\n",
        "x.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std()  # Menghitung mean dan standar deviasi untuk fitur pertama (kolom pertama) di seluruh input batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std()  # Menghitung mean dan standar deviasi untuk fitur dari satu input di dalam batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4198, val loss 2.4340\n",
            "step 400: train loss 2.3503, val loss 2.3567\n",
            "step 500: train loss 2.2970, val loss 2.3136\n",
            "step 600: train loss 2.2410, val loss 2.2506\n",
            "step 700: train loss 2.2062, val loss 2.2198\n",
            "step 800: train loss 2.1638, val loss 2.1871\n",
            "step 900: train loss 2.1232, val loss 2.1494\n",
            "step 1000: train loss 2.1020, val loss 2.1293\n",
            "step 1100: train loss 2.0704, val loss 2.1196\n",
            "step 1200: train loss 2.0382, val loss 2.0798\n",
            "step 1300: train loss 2.0249, val loss 2.0640\n",
            "step 1400: train loss 1.9922, val loss 2.0354\n",
            "step 1500: train loss 1.9707, val loss 2.0308\n",
            "step 1600: train loss 1.9614, val loss 2.0474\n",
            "step 1700: train loss 1.9393, val loss 2.0130\n",
            "step 1800: train loss 1.9070, val loss 1.9943\n",
            "step 1900: train loss 1.9057, val loss 1.9871\n",
            "step 2000: train loss 1.8834, val loss 1.9954\n",
            "step 2100: train loss 1.8719, val loss 1.9758\n",
            "step 2200: train loss 1.8582, val loss 1.9623\n",
            "step 2300: train loss 1.8546, val loss 1.9517\n",
            "step 2400: train loss 1.8410, val loss 1.9476\n",
            "step 2500: train loss 1.8167, val loss 1.9455\n",
            "step 2600: train loss 1.8263, val loss 1.9401\n",
            "step 2700: train loss 1.8108, val loss 1.9340\n",
            "step 2800: train loss 1.8040, val loss 1.9247\n",
            "step 2900: train loss 1.8044, val loss 1.9304\n",
            "step 3000: train loss 1.7963, val loss 1.9242\n",
            "step 3100: train loss 1.7687, val loss 1.9147\n",
            "step 3200: train loss 1.7547, val loss 1.9102\n",
            "step 3300: train loss 1.7557, val loss 1.9037\n",
            "step 3400: train loss 1.7547, val loss 1.8946\n",
            "step 3500: train loss 1.7385, val loss 1.8968\n",
            "step 3600: train loss 1.7260, val loss 1.8914\n",
            "step 3700: train loss 1.7257, val loss 1.8808\n",
            "step 3800: train loss 1.7204, val loss 1.8919\n",
            "step 3900: train loss 1.7215, val loss 1.8788\n",
            "step 4000: train loss 1.7146, val loss 1.8639\n",
            "step 4100: train loss 1.7095, val loss 1.8724\n",
            "step 4200: train loss 1.7079, val loss 1.8707\n",
            "step 4300: train loss 1.7035, val loss 1.8502\n",
            "step 4400: train loss 1.7043, val loss 1.8693\n",
            "step 4500: train loss 1.6914, val loss 1.8522\n",
            "step 4600: train loss 1.6853, val loss 1.8357\n",
            "step 4700: train loss 1.6862, val loss 1.8483\n",
            "step 4800: train loss 1.6671, val loss 1.8434\n",
            "step 4900: train loss 1.6736, val loss 1.8415\n",
            "step 4999: train loss 1.6635, val loss 1.8226\n",
            "\n",
            "FlY BOLINGLO:\n",
            "Them thrumply towiter arts the\n",
            "muscue rike begatt the sea it\n",
            "What satell in rowers that some than othis Marrity.\n",
            "\n",
            "LUCENTVO:\n",
            "But userman these that, where can is not diesty rege;\n",
            "What and see to not. But's eyes. What?\n",
            "\n",
            "JOHN MARGARET:\n",
            "Than up I wark, what out, I ever of and love,\n",
            "one these do sponce, vois I me;\n",
            "But my pray sape to ries all to the not erralied in may.\n",
            "\n",
            "BENVOLIO:\n",
            "To spits as stold's bewear I would and say mesby all\n",
            "on sworn make he anough\n",
            "As cousins the solle, whose be my conforeful may lie them yet\n",
            "nobe allimely untraled to be thre I say be,\n",
            "Notham a brotes theme an make come,\n",
            "And that his reach to the duke ento\n",
            "the grmeants bell! and now there king-liff-or grief?\n",
            "\n",
            "GLOUCESTER:\n",
            "All the bettle dreene, for To his like thou thron!\n",
            "\n",
            "MENENIUS:\n",
            "Then, if I knom her all.\n",
            "My lord, but terruly friend\n",
            "Rish of the ploceiness and wilt tends sure?\n",
            "Is you knows a fasir wead\n",
            "That with him my spaut,\n",
            "I shall not tas where's not, becomity; my coulds sting,\n",
            "then the wit be dong to tyget our hereefore,\n",
            "Who strop me, mend here, if agains, bitten, thy lack.\n",
            "The but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\n",
            "How the enclady: It you and how,\n",
            "I am in him, And ladderle:\n",
            "Their hand whose wife, it my hithre,\n",
            "Roman and where sposs gives'd you.\n",
            "\n",
            "TROMIOLANUS:\n",
            "But livants you great, I shom mistrot come, for to she to lot\n",
            "for smy to men ventry mehus. Gazise;\n",
            "Full't were some the cause, and stouch set,\n",
            "Or promises, which a kingsasted to your gove them; and sterrer,\n",
            "And that wae love him.\n",
            "\n",
            "BRUTUS:\n",
            "You shape with these sweet.\n",
            "\n",
            "CORTENGONO:\n",
            "Lo, where 'twon elmes, 'morth young agres;\n",
            "Sir, azavoust to striel accurded we missery sets crave.\n",
            "\n",
            "ANGOLUM:\n",
            "For is Henry to have gleise the dreason\n",
            "That I ant shorfold wefth their servy in enscy.\n",
            "\n",
            "ISABELLA:\n",
            "O, I better you eyse such formfetrews.\n",
            "\n",
            "BUCKINGHARENT:\n",
            "Qead my lightle this righanneds flase them\n",
            "Wam which an take was our some pleasurs,\n",
            "Lovisoname to me, then fult me?--have it?\n",
            "\n",
            "HENRY BOLINGBROY:\n",
            "That wha\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 16 # Banyaknya urutan yang diproses secara paralel\n",
        "block_size = 32 # Panjang konteks maksimum untuk prediksi\n",
        "max_iters = 5000 # Jumlah iterasi pelatihan maksimal\n",
        "eval_interval = 100 # Interval evaluasi setiap berapa iterasi\n",
        "learning_rate = 1e-3 # Laju pembelajaran\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # Menentukan perangkat (GPU jika tersedia)\n",
        "eval_iters = 200 # Jumlah iterasi untuk evaluasi\n",
        "n_embd = 64 # Dimensi embedding untuk token dan posisi\n",
        "n_head = 4 # Jumlah kepala dalam multi-head attention\n",
        "n_layer = 4 # Jumlah layer Transformer\n",
        "dropout = 0.0 # Tingkat dropout (regularisasi)\n",
        "\n",
        "# ------------\n",
        "\n",
        "# Menetapkan seed untuk memastikan eksperimen yang dapat direproduksi\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Membaca teks dari file input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Menyusun semua karakter unik dalam teks\n",
        "chars = sorted(list(set(text))) # Set untuk menghapus duplikasi, di-sort\n",
        "vocab_size = len(chars) # Ukuran kosakata, jumlah karakter unik dalam teks\n",
        "\n",
        "# Membuat pemetaan dari karakter ke integer dan sebaliknya\n",
        "stoi = { ch:i for i,ch in enumerate(chars) } # String ke integer\n",
        "itos = { i:ch for i,ch in enumerate(chars) } # Integer ke string\n",
        "\n",
        "# Fungsi encoder dan decoder\n",
        "encode = lambda s: [stoi[c] for c in s] # Mengubah string menjadi daftar indeks integer\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # Mengubah daftar indeks integer kembali menjadi string\n",
        "\n",
        "# Membagi data menjadi data pelatihan dan validasi\n",
        "data = torch.tensor(encode(text), dtype=torch.long) # Mengubah teks menjadi tensor\n",
        "n = int(0.9*len(data)) # 90% data untuk pelatihan\n",
        "train_data = data[:n] # Data pelatihan\n",
        "val_data = data[n:] # Data validasi\n",
        "\n",
        "# Fungsi untuk mengambil batch data\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data # Memilih data sesuai split (train/val)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Menghasilkan indeks acak untuk batch\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix]) # Ambil batch input (ukuran batch x block_size)\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) # Ambil target yang bergeser 1 langkah dari x\n",
        "    x, y = x.to(device), y.to(device) # Memindahkan batch ke perangkat (GPU/CPU)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval() # Menetapkan model ke mode evaluasi\n",
        "    for split in ['train', 'val']: # Evaluasi pada data train dan val\n",
        "        losses = torch.zeros(eval_iters) # Membuat tensor kosong untuk menyimpan loss\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split) # Mengambil batch untuk evaluasi\n",
        "            logits, loss = model(X, Y) # Menghitung logits dan loss\n",
        "            losses[k] = loss.item() # Menyimpan loss\n",
        "        out[split] = losses.mean() # Menghitung rata-rata loss\n",
        "    model.train() # Mengembalikan model ke mode pelatihan\n",
        "    return out\n",
        "\n",
        "# Head (Self-Attention)\n",
        "class Head(nn.Module):\n",
        "    \"\"\"Satu kepala self-attention\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False) # Layer untuk key\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False) # Layer untuk query\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False) # Layer untuk value\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) # Matriks segitiga bawah untuk masking\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout untuk regularisasi\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)   # Menghitung key\n",
        "        q = self.query(x) # Menghitung query\n",
        "        # Menghitung skor perhatian (affinities)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # Produk titik antara query dan key\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # Masking dengan matriks segitiga bawah\n",
        "        wei = F.softmax(wei, dim=-1) # Softmax untuk mendapatkan distribusi probabilitas\n",
        "        wei = self.dropout(wei) # Dropout pada bobot perhatian\n",
        "        # Agregasi berbobot nilai-nilai\n",
        "        v = self.value(x) # Menghitung value\n",
        "        out = wei @ v # Perkalian matriks antara bobot perhatian dan value\n",
        "        return out\n",
        "\n",
        "# MultiHeadAttention (Beberapa Kepala Self-Attention)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Beberapa kepala self-attention secara paralel\"\"\"\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)]) # Membuat beberapa kepala\n",
        "        self.proj = nn.Linear(n_embd, n_embd) # Proyeksi hasil dari semua kepala\n",
        "        self.dropout = nn.Dropout(dropout) # Dropout untuk regularisasi\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # Menggabungkan output dari semua kepala\n",
        "        out = self.dropout(self.proj(out)) # Proyeksi dan dropout\n",
        "        return out\n",
        "\n",
        "# FeedForward (Layer Linear dengan Aktivasi Non-Linear)\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\"Layer linear yang diikuti dengan non-linearitas\"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd), # Mengubah dimensi menjadi 4 kali lipat\n",
        "            nn.ReLU(), # Fungsi aktivasi ReLU\n",
        "            nn.Linear(4 * n_embd, n_embd), # Mengembalikan dimensi ke n_embd\n",
        "            nn.Dropout(dropout), # Dropout untuk regularisasi\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x) # Forward pass melalui jaringan\n",
        "\n",
        "# Block Transformer\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Block Transformer: komunikasi diikuti dengan komputasi\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head # Menentukan ukuran setiap kepala\n",
        "        self.sa = MultiHeadAttention(n_head, head_size) # Multi-head attention\n",
        "        self.ffwd = FeedFoward(n_embd) # Feedforward\n",
        "        self.ln1 = nn.LayerNorm(n_embd) # Normalisasi layer pertama\n",
        "        self.ln2 = nn.LayerNorm(n_embd) # Normalisasi layer kedua\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x)) # Residual connection setelah self-attention\n",
        "        x = x + self.ffwd(self.ln2(x)) # Residual connection setelah feedforward\n",
        "        return x\n",
        "\n",
        "# Bigram Language Model menggunakan Transformer\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # Tabel embedding untuk token\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # Tabel embedding untuk posisi\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # Beberapa blok Transformer\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # Layer normalisasi terakhir\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # Layer untuk memetakan kembali ke kosakata\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # Mendapatkan embedding untuk token\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # Mendapatkan embedding untuk posisi\n",
        "        x = tok_emb + pos_emb # Menambahkan embedding token dan posisi\n",
        "        x = self.blocks(x) # Melalui blok Transformer\n",
        "        x = self.ln_f(x) # Layer normalisasi terakhir\n",
        "        logits = self.lm_head(x) # Mendapatkan logits\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C) # Merubah bentuk untuk cross-entropy loss\n",
        "            targets = targets.view(B*T) # Merubah bentuk target\n",
        "            loss = F.cross_entropy(logits, targets) # Menghitung loss\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:] # Mengambil hanya block_size token terakhir\n",
        "            logits, loss = self(idx_cond) # Mendapatkan prediksi\n",
        "            logits = logits[:, -1, :] # Fokus pada prediksi untuk token terakhir\n",
        "            probs = F.softmax(logits, dim=-1) # Softmax untuk probabilitas\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # Sampling token berikutnya\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # Menambahkan token baru ke urutan\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# Menampilkan jumlah parameter dalam model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Membuat optimizer PyTorch\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss() # Evaluasi loss\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # Mengambil batch data pelatihan\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # Menghitung loss dan memperbarui parameter model\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Menghasilkan teks dari model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device) # Mulai dengan konteks kosong\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist())) # Menghasilkan 2000 token\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}