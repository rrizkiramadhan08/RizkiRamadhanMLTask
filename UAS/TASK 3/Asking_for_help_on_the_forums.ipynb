{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pCoNJEfs_0B"
      },
      "source": [
        "# Asking for help on the forums"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBl0cYLqs_0D"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc-ustAss_0D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dcaee9-3b9b-4742-e402-283c9e643bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (4.25.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oikZpz4Ts_0E"
      },
      "outputs": [],
      "source": [
        "# Mengimpor tokenizer otomatis dan model umum.\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        " # Nama model yang akan digunakan.\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "# Memuat tokenizer berdasarkan model checkpoint.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# Memuat model AutoModel dari checkpoint.\n",
        "model = AutoModel.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5z4rrf2s_0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf0240b-c7d1-4510-fb3a-a1dead683d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# Mengimpor model khusus untuk klasifikasi urutan\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "# Nama model yang digunakan untuk klasifikasi urutan.\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        " # Memuat tokenizer untuk klasifikasi urutan.\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# Memuat model untuk klasifikasi dengan 2 label.\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2) # Assuming 2 labels for demonstration\n",
        "\n",
        "# Mendefinisikan teks panjang sebagai input.\n",
        "text = \"\"\"\n",
        "Generation One is a retroactive term for the Transformers characters that\n",
        "appeared between 1984 and 1993. The Transformers began with the 1980s Japanese\n",
        "toy lines Micro Change and Diaclone. They presented robots able to transform\n",
        "into everyday vehicles, electronic items or weapons. Hasbro bought the Micro\n",
        "Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by\n",
        "Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall\n",
        "story, and gave the task of creating the characthers to writer Dennis O'Neil.\n",
        "Unhappy with O'Neil's work (although O'Neil created the name \"Optimus Prime\"),\n",
        "Shooter chose Bob Budiansky to create the characters.\n",
        "\n",
        "The Transformers mecha were largely designed by Shōji Kawamori, the creator of\n",
        "the Japanese mecha anime franchise Macross (which was adapted into the Robotech\n",
        "franchise in North America). Kawamori came up with the idea of transforming\n",
        "mechs while working on the Diaclone and Macross franchises in the early 1980s\n",
        "(such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs\n",
        "later providing the basis for Transformers.\n",
        "\n",
        "The primary concept of Generation One is that the heroic Optimus Prime, the\n",
        "villainous Megatron, and their finest soldiers crash land on pre-historic Earth\n",
        "in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through\n",
        "the Neutral zone as an effect of the war. The Marvel comic was originally part\n",
        "of the main Marvel Universe, with appearances from Spider-Man and Nick Fury,\n",
        "plus some cameos, as well as a visit to the Savage Land.\n",
        "\n",
        "The Transformers TV series began around the same time. Produced by Sunbow\n",
        "Productions and Marvel Productions, later Hasbro Productions, from the start it\n",
        "contradicted Budiansky's backstories. The TV series shows the Autobots looking\n",
        "for new energy sources, and crash landing as the Decepticons attack. Marvel\n",
        "interpreted the Autobots as destroying a rogue asteroid approaching Cybertron.\n",
        "Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a\n",
        "stalemate during his absence, but in the comic book he attempts to take command\n",
        "of the Decepticons. The TV series would also differ wildly from the origins\n",
        "Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire\n",
        "(known as Skyfire on TV), the Constructicons (who combine to form\n",
        "Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on\n",
        "that Prime wields the Creation Matrix, which gives life to machines. In the\n",
        "second season, the two-part episode The Key to Vector Sigma introduced the\n",
        "ancient Vector Sigma computer, which served the same original purpose as the\n",
        "Creation Matrix (giving life to Transformers), and its guardian Alpha Trion.\n",
        "\"\"\"\n",
        "# Mengencode teks menjadi tensor PyTorch, dengan panjang maksimal 512 token.\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "# Memberikan input ke model dan mendapatkan keluaran.\n",
        "\n",
        "outputs = model(**inputs)\n",
        "# Mengakses logit dari keluaran model.\n",
        "logits = outputs.logits"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}